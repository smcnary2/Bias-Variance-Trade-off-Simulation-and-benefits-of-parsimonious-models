# Bias-Variance-Trade-off-Simulation-and-benefits-of-parsimonious-models
## Overview
This project demonstrates the bias–variance tradeoff using simulated regression data with a known quadratic data-generating process. By comparing a simple linear model with a high-degree polynomial model, the experiment illustrates how model complexity and training set size affect prediction bias, variance, and mean squared error (MSE).

The script is designed to be rerun with different parameter settings (e.g., training size or polynomial degree) to observe how variance changes.

## Data Generation
The true underlying relationship between the predictor and response is:

f(x) = x² + x + 1

Observed responses are generated by adding Gaussian noise:

y = f(x) + ε, ε ~ N(0, σ²)

where σ = 1. Predictor values are sampled uniformly from the interval [0, 2].

Because the true function is quadratic, it provides a clear benchmark for evaluating model bias and variance.


## Models Compared

### Biased Model: Linear Regression
A linear regression model is fit to the data:

y ~ x

This model is intentionally misspecified, as it cannot represent the quadratic structure of the true function. As a result, it exhibits **high bias** but relatively **low variance** across repeated samples.


### High-Variance Model: Polynomial Regression
A polynomial regression model of high degree is fit:

y ~ poly(x, d)

with d set to a large value (e.g., d = 8).

This model is flexible enough to closely fit the training data, including random noise. While this reduces bias, it leads to **high variance**, particularly when training samples are small.


## Simulation Procedure
To estimate bias and variance, the following simulation is performed:

1. A pool of candidate x-values is generated.
2. Multiple training datasets are sampled from this pool.
3. Both models are fit to each training dataset.
4. Predictions are made on a fixed, dense test grid.
5. Predictions are aggregated across simulations.

This repeated-sampling approach allows bias and variance to be estimated empirically.


## Bias–Variance Decomposition

For each test point:
- **Bias²** is computed as the squared difference between the mean predicted value and the true function value.
- **Variance** is computed as the variance of predictions across simulations.
- **Mean Squared Error (MSE)** combines bias², variance, and irreducible noise.

Metrics are averaged across the test domain to summarize model performance.


## Results
The linear model consistently shows higher bias but lower variance, while the polynomial model substantially reduces bias at the cost of increased variance. Mean squared error reflects this tradeoff, illustrating that reducing bias alone does not guarantee better predictive performance.

When the training set size is small, variance dominates for the high-degree polynomial model. As the training set size increases, variance decreases and model predictions become more stable.


## Visualization
Plots of bias², variance, and MSE across the input domain visually reinforce the bias–variance tradeoff. In particular:

- Bias² is larger for the linear model across most values of x.
- Variance is significantly larger for the polynomial model.
- MSE reflects the balance between these two components.

An additional comparison plot of average variance highlights how model complexity affects prediction stability.


## Key Insight
The bias–variance tradeoff is a fundamental concept in statistical learning. Models that are too simple may underfit and exhibit high bias, while overly flexible models may overfit and exhibit high variance. Effective modeling requires balancing these two sources of error, often through appropriate model selection and sufficient training data.


## Extensions
Possible extensions of this analysis include:
- Varying polynomial degree systematically
- Exploring the effect of training set size on variance
- Applying regularization techniques such as ridge regression
- Comparing polynomial models with splines or generalized additive models


## Tools
- R (base functions only)
